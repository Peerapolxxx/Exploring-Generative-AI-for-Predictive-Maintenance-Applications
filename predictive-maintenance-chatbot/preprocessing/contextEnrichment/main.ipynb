{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from contextEnrichment_function.gpt_contextualized import gpt_batch_process\n",
    "from contextEnrichment_function.anthropic_contextualized import anthropic_batch_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_contextEnrichment(prepared_data_dir):\n",
    "\n",
    "    # อ่านรายการโรงงานจากไฟล์ CSV\n",
    "    file_name = \"plant_list.csv\"\n",
    "    plant_list_file = os.path.join(prepared_data_dir, file_name)\n",
    "    if os.path.isfile(plant_list_file):\n",
    "        plant_df = pd.read_csv(plant_list_file)\n",
    "\n",
    "    else:\n",
    "        print(f\":: Failed ❌\")\n",
    "        print(f\"File not found: {plant_list_file}\")\n",
    "        return  # ออกจากฟังก์ชันถ้าไม่พบไฟล์\n",
    "\n",
    "    # วนลูปผ่านแต่ละโรงงาน\n",
    "    for _, plant_row in plant_df.iterrows():\n",
    "        plant_tag = plant_row[\"PLANT_TAG\"]\n",
    "        plant_name = plant_row[\"PLANT_NAME\"]\n",
    "\n",
    "        # อ่านรายการเครื่องจักรของแต่ละโรงงาน\n",
    "        file_name = \"machine_list.csv\"\n",
    "        machine_list_file = os.path.join(prepared_data_dir, plant_tag, file_name)\n",
    "        if os.path.isfile(machine_list_file):\n",
    "            machine_df = pd.read_csv(machine_list_file)\n",
    "        else:\n",
    "            print(f\":: Failed ❌\")\n",
    "            print(f\"File not found: {machine_list_file}\")\n",
    "            break\n",
    "\n",
    "        # วนลูปผ่านแต่ละเครื่องจักร\n",
    "        for idx, machine_row in machine_df.iterrows():\n",
    "            machine_tag = machine_row[\"MACHINE_TAG\"]\n",
    "            machine_name = machine_row[\"MACHINE_NAME\"]\n",
    "            print(\"\\n\" + \"=\" * 100)\n",
    "            print(\n",
    "                f\"#{idx+1} Processing data for {plant_name} (TAG: {plant_tag}) - {machine_name} (TAG: {machine_tag})\"\n",
    "            )\n",
    "            print(\"=\" * 100)\n",
    "\n",
    "            # เพิ่มบริบทให้กับเนื้อหาชิ้นต่างๆ\n",
    "            file_name = f\"{plant_tag}_{machine_tag}_chunks.json\"\n",
    "            print(f\"\\n>> Add contextualization to chunk content - File: {file_name}\")\n",
    "            chunks_file = os.path.join(\n",
    "                prepared_data_dir, plant_tag, machine_tag, file_name\n",
    "            )\n",
    "            if os.path.isfile(chunks_file):\n",
    "\n",
    "                # Read the JSON file\n",
    "                with open(chunks_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                    chunks_data = json.load(file)\n",
    "                    print(f\":: Read the Chunks data JSON Complete ✔️ - Path: {chunks_file}\")\n",
    "\n",
    "                # เรียกใช้งานฟังก์ชั่นเพื่อใช้ anthropic AI ในการสร้างบริบท\n",
    "                contextualized_data = anthropic_batch_process(chunks_data)\n",
    "\n",
    "                # เขียนข้อมูลที่ประมวลผลแล้วกลับลงไฟล์\n",
    "                contextualized_full_file_path = chunks_file.replace(\n",
    "                    \".json\", \"_contextualized.json\"\n",
    "                )\n",
    "                with open(contextualized_full_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    json.dump(contextualized_data, file, ensure_ascii=False, indent=2)\n",
    "                print(f\":: Writing processed data Complete ✔️\")\n",
    "\n",
    "            else:\n",
    "                print(f\":: Failed ❌\")\n",
    "                print(f\"File not found: {chunks_file}\")\n",
    "\n",
    "            print(\"=\" * 100)\n",
    "            print(\"Complete all Process\")\n",
    "            print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "#1 Processing data for Natural Gas Processing Plant (TAG: PLANT_01) - Sale Gas Compressor (TAG: COMP_SG01)\n",
      "====================================================================================================\n",
      "\n",
      ">> Add contextualization to chunk content - File: PLANT_01_COMP_SG01_chunks.json\n",
      ":: Read the Chunks data JSON Complete ✔️ - Path: D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\\predictive-maintenance-chatbot\\data\\prepared_data\\PLANT_01\\COMP_SG01\\PLANT_01_COMP_SG01_chunks.json\n",
      ":: Read the Corpus source Complete ✔️ - Path: D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\\predictive-maintenance-chatbot\\data\\prepared_data\\PLANT_01\\COMP_SG01\\PLANT_01_COMP_SG01_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: Processing chunks: 100%|██████████| 178/178 [1:09:57<00:00, 23.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Writing processed data Complete ✔️\n",
      "====================================================================================================\n",
      "Complete all Process\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "#1 Processing data for Everflow Utility Plant (TAG: PLANT_02) - Dual Fuel Generator A (TAG: GEN_DF_01)\n",
      "====================================================================================================\n",
      "\n",
      ">> Add contextualization to chunk content - File: PLANT_02_GEN_DF_01_chunks.json\n",
      ":: Read the Chunks data JSON Complete ✔️ - Path: D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\\predictive-maintenance-chatbot\\data\\prepared_data\\PLANT_02\\GEN_DF_01\\PLANT_02_GEN_DF_01_chunks.json\n",
      ":: Read the Corpus source Complete ✔️ - Path: D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\\predictive-maintenance-chatbot\\data\\prepared_data\\PLANT_02\\GEN_DF_01\\PLANT_02_GEN_DF_01_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: Processing chunks: 100%|██████████| 59/59 [16:41<00:00, 16.97s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Writing processed data Complete ✔️\n",
      "====================================================================================================\n",
      "Complete all Process\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "#2 Processing data for Everflow Utility Plant (TAG: PLANT_02) - Dual Fuel Generator B (TAG: GEN_DF_02)\n",
      "====================================================================================================\n",
      "\n",
      ">> Add contextualization to chunk content - File: PLANT_02_GEN_DF_02_chunks.json\n",
      ":: Read the Chunks data JSON Complete ✔️ - Path: D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\\predictive-maintenance-chatbot\\data\\prepared_data\\PLANT_02\\GEN_DF_02\\PLANT_02_GEN_DF_02_chunks.json\n",
      ":: Read the Corpus source Complete ✔️ - Path: D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\\predictive-maintenance-chatbot\\data\\prepared_data\\PLANT_02\\GEN_DF_02\\PLANT_02_GEN_DF_02_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: Processing chunks: 100%|██████████| 59/59 [07:47<00:00,  7.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Writing processed data Complete ✔️\n",
      "====================================================================================================\n",
      "Complete all Process\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "#3 Processing data for Everflow Utility Plant (TAG: PLANT_02) - Produce Water Injection Pump A (TAG: PMP_WI_01)\n",
      "====================================================================================================\n",
      "\n",
      ">> Add contextualization to chunk content - File: PLANT_02_PMP_WI_01_chunks.json\n",
      ":: Read the Chunks data JSON Complete ✔️ - Path: D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\\predictive-maintenance-chatbot\\data\\prepared_data\\PLANT_02\\PMP_WI_01\\PLANT_02_PMP_WI_01_chunks.json\n",
      ":: Read the Corpus source Complete ✔️ - Path: D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\\predictive-maintenance-chatbot\\data\\prepared_data\\PLANT_02\\PMP_WI_01\\PLANT_02_PMP_WI_01_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: Processing chunks: 100%|██████████| 10/10 [00:17<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Writing processed data Complete ✔️\n",
      "====================================================================================================\n",
      "Complete all Process\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "#4 Processing data for Everflow Utility Plant (TAG: PLANT_02) - Produce Water Injection Pump B (TAG: PMP_WI_02)\n",
      "====================================================================================================\n",
      "\n",
      ">> Add contextualization to chunk content - File: PLANT_02_PMP_WI_02_chunks.json\n",
      ":: Read the Chunks data JSON Complete ✔️ - Path: D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\\predictive-maintenance-chatbot\\data\\prepared_data\\PLANT_02\\PMP_WI_02\\PLANT_02_PMP_WI_02_chunks.json\n",
      ":: Read the Corpus source Complete ✔️ - Path: D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\\predictive-maintenance-chatbot\\data\\prepared_data\\PLANT_02\\PMP_WI_02\\PLANT_02_PMP_WI_02_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: Processing chunks: 100%|██████████| 10/10 [00:15<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Writing processed data Complete ✔️\n",
      "====================================================================================================\n",
      "Complete all Process\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "#5 Processing data for Everflow Utility Plant (TAG: PLANT_02) - Produce Water Injection Pump C (TAG: PMP_WI_03)\n",
      "====================================================================================================\n",
      "\n",
      ">> Add contextualization to chunk content - File: PLANT_02_PMP_WI_03_chunks.json\n",
      ":: Read the Chunks data JSON Complete ✔️ - Path: D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\\predictive-maintenance-chatbot\\data\\prepared_data\\PLANT_02\\PMP_WI_03\\PLANT_02_PMP_WI_03_chunks.json\n",
      ":: Read the Corpus source Complete ✔️ - Path: D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\\predictive-maintenance-chatbot\\data\\prepared_data\\PLANT_02\\PMP_WI_03\\PLANT_02_PMP_WI_03_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: Processing chunks: 100%|██████████| 10/10 [00:17<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Writing processed data Complete ✔️\n",
      "====================================================================================================\n",
      "Complete all Process\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "#6 Processing data for Everflow Utility Plant (TAG: PLANT_02) - Water process (TAG: SYS_WP_01)\n",
      "====================================================================================================\n",
      "\n",
      ">> Add contextualization to chunk content - File: PLANT_02_SYS_WP_01_chunks.json\n",
      ":: Read the Chunks data JSON Complete ✔️ - Path: D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\\predictive-maintenance-chatbot\\data\\prepared_data\\PLANT_02\\SYS_WP_01\\PLANT_02_SYS_WP_01_chunks.json\n",
      ":: Read the Corpus source Complete ✔️ - Path: D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\\predictive-maintenance-chatbot\\data\\prepared_data\\PLANT_02\\SYS_WP_01\\PLANT_02_SYS_WP_01_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: Processing chunks: 100%|██████████| 8/8 [00:13<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Writing processed data Complete ✔️\n",
      "====================================================================================================\n",
      "Complete all Process\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# กำหนด root directory หลักของโปรเจค\n",
    "ROOT_DIRECTORY = \"D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\"\n",
    "\n",
    "# กำหนดชื่อโฟลเดอร์ย่อยต่างๆ\n",
    "PROJECT_DIRECTORY = \"predictive-maintenance-chatbot\"    # โฟลเดอร์โปรเจค\n",
    "DATA_ROOT_DIRECTORY = \"data\"                            # โฟลเดอร์หลักสำหรับเก็บข้อมูล\n",
    "PREPARED_DATA_DIRECTORY = \"prepared_data\"               # โฟลเดอร์สำหรับข้อมูลที่ประมวลผลแล้ว\n",
    "\n",
    "# สร้าง path เต็มสำหรับโฟลเดอร์ข้อมูลที่ประมวลผลแล้ว\n",
    "prepared_data_dir = os.path.join(\n",
    "    ROOT_DIRECTORY, PROJECT_DIRECTORY, DATA_ROOT_DIRECTORY, PREPARED_DATA_DIRECTORY\n",
    ")\n",
    "\n",
    "# Call the pipeline_numeric2text function\n",
    "pipeline_contextEnrichment(prepared_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_json = r\"D:\\Data_sci_internship\\Exploring Generative AI for Predictive Maintenance Applications\\predictive-maintenance-chatbot\\data\\prepared_data\\PLANT_02\\SYSTEM\\PLANT_02_SYSTEM_chunks_contextualized.json\"\n",
    "# with open(data_json, \"r\", encoding=\"utf-8\") as file:\n",
    "#     contextualized_data = json.load(file)\n",
    "#     print(\":: Complete ✔️\")\n",
    "\n",
    "# for data in contextualized_data:\n",
    "#     chunks = data[\"chunks\"]\n",
    "#     corpus_source_file = data[\"corpus_source\"]\n",
    "\n",
    "#     if os.path.isfile(corpus_source_file):\n",
    "#         with open(corpus_source_file, \"r\") as file:\n",
    "#             corpus_content = file.read()\n",
    "#             print(f\":: Read the Corpus source Complete ✔️ - Path: {corpus_source_file}\")\n",
    "#     else:\n",
    "#         print(f\":: Failed ❌ - File not found: {corpus_source_file}\")\n",
    "\n",
    "#     for chunk in chunks:\n",
    "#         chunk_content = chunk[\"content\"]\n",
    "#         contextualized_content = chunk[\"contextualized_content\"]\n",
    "#         print(\"=\" * 100)\n",
    "#         print(contextualized_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PdM_RschAI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
